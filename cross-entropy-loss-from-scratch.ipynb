{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30761,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Implementing Cross-Entropy Loss from Scratch\nimport numpy as np\nimport torch\n\n# Create a tensor of logits (raw scores before softmax)\nlogits = torch.tensor([ [2.0, 1.0, 0.1],[0.5, 2.5, 0.3]])\nprint(f'logits:{logits}')\n\n# Interpretation\n# These are the raw scores (logits) from the model for two samples and three classes.\n# Each row corresponds to one sample, and each value in the row is the unnormalized score for each class.\n# For example, for the first sample:\n# Class 0 has a logit of 2.0,\n# Class 1 has a logit of 1.0,\n# Class 2 has a logit of 0.1.\n# For the second sample, Class 1 has the highest logit (2.5).","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-09-08T17:59:37.346280Z","iopub.execute_input":"2024-09-08T17:59:37.347583Z","iopub.status.idle":"2024-09-08T17:59:37.462659Z","shell.execute_reply.started":"2024-09-08T17:59:37.347520Z","shell.execute_reply":"2024-09-08T17:59:37.461302Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"logits:tensor([[2.0000, 1.0000, 0.1000],\n        [0.5000, 2.5000, 0.3000]])\n","output_type":"stream"}]},{"cell_type":"code","source":"# Implementing Cross-Entropy Loss from Scratch\n\n# apply the softmax function to convert these logits into probabilities\nsoftmax = torch.nn.functional.softmax(logits, dim=1)\n\nprint(\"\\nSoftmax Probabilities:\")\nprint(softmax)\n\n# Interpretation\n# The softmax function converts logits into probabilities that sum to 1 across each row (for each sample).\n# For the first sample:\n# Class 0 has a probability of 0.6590,\n# Class 1 has a probability of 0.2424,\n# Class 2 has a probability of 0.0986.\n\n# For the second sample:\n# Class 1 has the highest probability of 0.8025","metadata":{"execution":{"iopub.status.busy":"2024-09-08T18:22:22.705618Z","iopub.execute_input":"2024-09-08T18:22:22.706124Z","iopub.status.idle":"2024-09-08T18:22:22.715556Z","shell.execute_reply.started":"2024-09-08T18:22:22.706080Z","shell.execute_reply":"2024-09-08T18:22:22.714253Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"\nSoftmax Probabilities:\ntensor([[0.6590, 0.2424, 0.0986],\n        [0.1086, 0.8025, 0.0889]])\n","output_type":"stream"}]},{"cell_type":"code","source":"# Implementing Cross-Entropy Loss from Scratch\n# take the logarithm of the softmax probabilities\nlog_probs = torch.log(softmax)\nprint(log_probs)\n\n# Interpretation\n# The log function is applied to the softmax probabilities. \n# The log of a number between 0 and 1 is always negative.\n# These log values tell us how confident the model is in its predictions. \n# Larger negative values (e.g., -2.3180) correspond to low probabilities.\n# Values closer to 0 (e.g., -0.2412) correspond to higher probabilities.","metadata":{"execution":{"iopub.status.busy":"2024-09-08T18:07:19.161950Z","iopub.execute_input":"2024-09-08T18:07:19.163590Z","iopub.status.idle":"2024-09-08T18:07:19.181886Z","shell.execute_reply.started":"2024-09-08T18:07:19.163513Z","shell.execute_reply":"2024-09-08T18:07:19.180106Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"tensor([[-0.4170, -1.4170, -2.3170],\n        [-2.2200, -0.2200, -2.4200]])\n","output_type":"stream"}]},{"cell_type":"code","source":"# Implementing Cross-Entropy Loss from Scratch\n\n# assume that the true class labels for the two samples are 0 and 1, respectively\n# We will compute the negative log likelihood for these true labels.\n\n# Apply the Negative Logarithm (Compute the Loss)\ntrue_labels = torch.tensor([0,1])\n\n# Extract the log probabilities corresponding to the true labels\nnegative_log_likelihood = -log_probs[range(len(true_labels)), true_labels]\n\nprint(\"\\nNegative Log Likelihood for True Labels:\")\nprint(negative_log_likelihood)\n\n\n# Interpretation\n","metadata":{"execution":{"iopub.status.busy":"2024-09-08T18:11:15.006279Z","iopub.execute_input":"2024-09-08T18:11:15.006792Z","iopub.status.idle":"2024-09-08T18:11:15.017355Z","shell.execute_reply.started":"2024-09-08T18:11:15.006746Z","shell.execute_reply":"2024-09-08T18:11:15.015888Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"\nNegative Log Likelihood for True Labels:\ntensor([0.4170, 0.2200])\n","output_type":"stream"}]},{"cell_type":"code","source":"# Implementing Cross-Entropy Loss from Scratch\n# compute the average cross-entropy loss by averaging the negative log likelihood values\n\n# compute the average cross-entropy loss (mean of the negative log likelihoods)\ncross_entropy_loss = negative_log_likelihood.mean()\n\nprint(\"\\nCross-Entropy Loss:\")\nprint(cross_entropy_loss.item())\n\n# Interpretation\n# The cross-entropy loss is the mean of the negative log likelihoods for both samples: \n# (0.4170 + 0.2200) / 2 = 0.3185\n\n# This value represents the overall loss for the batch, and it is the value that the model tries to minimize during training.\n# A lower cross-entropy loss means the model is making better predictions (i.e., assigning higher probabilities to the correct classes).\n\n# the loss of 0.3291 suggests that the model is doing reasonably well, but there is still room for improvement.","metadata":{"execution":{"iopub.status.busy":"2024-09-08T18:13:34.101545Z","iopub.execute_input":"2024-09-08T18:13:34.102033Z","iopub.status.idle":"2024-09-08T18:13:34.111831Z","shell.execute_reply.started":"2024-09-08T18:13:34.101986Z","shell.execute_reply":"2024-09-08T18:13:34.110374Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"\nCross-Entropy Loss:\n0.3185397684574127\n","output_type":"stream"}]},{"cell_type":"code","source":"# Implementing Cross-Entropy Loss from Scratch\nimport torch\n\n# Step 1: Create a tensor of logits (raw scores before softmax)\nlogits = torch.tensor([[2.0, 1.0, 0.1],\n                       [0.5, 2.5, 0.3]])  # 2 samples, 3 classes\nprint(\"Logits:\")\nprint(logits)\n\n# Step 2: Apply softmax to logits to get probabilities\nsoftmax = torch.nn.functional.softmax(logits, dim=1)\nprint(\"\\nSoftmax Probabilities:\")\nprint(softmax)\n\n# Step 3: Take the log of the softmax probabilities\nlog_probs = torch.log(softmax)\nprint(\"\\nLog of Softmax Probabilities:\")\nprint(log_probs)\n\n# Step 4: True labels (0 and 1)\ntrue_labels = torch.tensor([0, 1])\n\n# Extract the log probabilities corresponding to the true labels\nnegative_log_likelihood = -log_probs[range(len(true_labels)), true_labels]\nprint(\"\\nNegative Log Likelihood for True Labels:\")\nprint(negative_log_likelihood)\n\n# Step 5: Compute the cross-entropy loss (mean of negative log likelihoods)\ncross_entropy_loss = negative_log_likelihood.mean()\nprint(\"\\nCross-Entropy Loss:\")\nprint(cross_entropy_loss.item())","metadata":{"execution":{"iopub.status.busy":"2024-09-08T18:32:23.358911Z","iopub.execute_input":"2024-09-08T18:32:23.359576Z","iopub.status.idle":"2024-09-08T18:32:23.380652Z","shell.execute_reply.started":"2024-09-08T18:32:23.359516Z","shell.execute_reply":"2024-09-08T18:32:23.379283Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"Logits:\ntensor([[2.0000, 1.0000, 0.1000],\n        [0.5000, 2.5000, 0.3000]])\n\nSoftmax Probabilities:\ntensor([[0.6590, 0.2424, 0.0986],\n        [0.1086, 0.8025, 0.0889]])\n\nLog of Softmax Probabilities:\ntensor([[-0.4170, -1.4170, -2.3170],\n        [-2.2200, -0.2200, -2.4200]])\n\nNegative Log Likelihood for True Labels:\ntensor([0.4170, 0.2200])\n\nCross-Entropy Loss:\n0.3185397684574127\n","output_type":"stream"}]}]}