{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"## Chapter 4: Under the Hood: Training a Digital Classifier","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from fastai.vision.all import *\nfrom fastai.vision.widgets import ImageClassifierCleaner\nfrom fastai.text.all import *\nfrom fastai.tabular.all import *\nfrom fastai.collab import *\nfrom fastai.basics import *\nfrom torchvision.models import resnet34\nimport ipywidgets as widgets\nfrom IPython.display import display, clear_output\nfrom ipywidgets import Dropdown, VBox, Button, Layout, Label, interact\nimport os\nimport pandas as pd\nfrom PIL import Image\nfrom pathlib import Path\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport shutil\nimport torch","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## use doc() to answer questions about a fastai method\n\n# What is untar_data\ndoc(untar_data)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# use untar_data to extract urls and return the paths \n# this creates the following path: '/root/.fastai/data/mnist_sample'\n# download samples of the NIST images of 3 or 7\npath = untar_data(URLs.MNIST_SAMPLE)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Set the base path\nPath.BASE_PATH = 'train'\n\n# Define the paths\ntrain_path = path/'train'\nvalid_path = path/'valid'\ncsv_path = path/'labels.csv'\n\n# Create directories if they don't exist\ntrain_path.mkdir(parents=True, exist_ok=True)\nvalid_path.mkdir(parents=True, exist_ok=True)\n\n# Create the labels.csv file (you can replace '...' with actual label data)\nlabels_data = pd.DataFrame({'file': [], 'label': []})\nlabels_data.to_csv(csv_path, index=False)\n\n# Check the updated directory structure\n## list folders for training, validation, and test data\npath.ls()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# list the training data folders\n(path/'train').ls()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sevens = (path/'train'/'7').ls().sorted()\nthrees = (path/'train'/'3').ls().sorted()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# list the threes (image files)\nthrees","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Goal: demonstrate what an image looks like to a computer ##\n# view an image of a three\n# the Image class comes from the Python Imaging Library (PIL)\nim3_path = threes[1]\nim3 = Image.open(im3_path)\nim3","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Goal: demonstrate what an image looks like to a computer ##\n# convert the image of the 3 to a NumPy array \n# ensure numpy is imported as np\n# [rows, columns]\n# index rows / columns 4 up to 10 (not included)\narray(im3)[4:10,4:10]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Goal: demonstrate what an image looks like to a computer ##\n# convert the image of the 3 to a PyTorch tensor\ntensor(im3)[4:10,4:10]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Goal Complete: This is what an image looks like to a computer ##\n# slice the array to pick just the part with the top digit in it\nim3_t = tensor(im3)\ndf = pd.DataFrame(im3_t[4:15,4:22])\ndf","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Goal Complete: This is what an image looks like to a computer ##\n# use a Pandas DataFrame to color-code the values with a gradient\n# 0's are white; 255's are black; shades of grey are between 0 and 255\ndf.style.set_properties(**{'font-size':'6pt'}).background_gradient('Greys')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Goal: create a very simple baseline model using averages\n# creat a tensor for all images of 3 and 7 in a directory\nseven_tensors = [tensor(Image.open(o)) for o in sevens]\nthree_tensors = [tensor(Image.open(o)) for o in threes]\nlen(three_tensors), len(seven_tensors)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Goal: create a very simple baseline model using averages\n# check one of the images using the fastai show_image() method\nshow_image(three_tensors[1])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Goal: create a very simple baseline model using averages\n# combine all of the images in the 3s and 7s lists into a single three-dimensional tensor\n# use the PyTorch stack() method to stack up individual tensors into a single tensor\n# cast the stacked tensor into a float and divide by 255 to get a number from 0 to 1\nstacked_sevens = torch.stack(seven_tensors).float()/255\nstacked_threes = torch.stack(three_tensors).float()/255","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Goal: create a very simple baseline model using averages\n# an important attribute of a tensor is its shape which tells you the length of each axis\n# we have 6,131 images, each of size 28Ã—28 pixels.\nstacked_threes.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Goal: create a very simple baseline model using averages\n# the length of a tensor's shape is  its rank\n# this is a rank-3 tensor\nlen(stacked_threes.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Goal: create a very simple baseline model using averages\n# get a tensor's rank directly with ndim\nstacked_threes.ndim","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Goal: create a very simple baseline model using average\n# determine the ideal 3 in our rank-3 tensor\n# calculate the mean of all the image tensors \n# for every pixel position, compute the average of that pixel over all images\nmean3 = stacked_threes.mean(0)\nshow_image(mean3)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Goal: create a very simple baseline model using average\n# determine the ideal 7 in our rank-3 tensor\nmean7 = stacked_sevens.mean(0)\nshow_image(mean7)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Goal: create a very simple baseline model using average\n# we will measure the distance from this aribrary '3' to our \"ideal 3\"\n\n# pick an arbitrary 3 \na_3 = stacked_threes[1]\nshow_image(a_3)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Goal: create a very simple baseline model using average\n# measuring distance\n# method 1: Mean Absolute Difference (a.k.k L1 norm) \ndist_3_abs = (a_3 - mean3).abs().mean()\n\n# method 2: Root Mean Squared Error (RRMSE)(a.k.a L2 norm)\ndist_3_sqr = ((a_3 - mean3)**2).mean().sqrt()\n\n# display the distances\ndist_3_abs, dist_3_sqr","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Goal: create a very simple baseline model using average\n# we will measure the distance from this aribrary '7' to our \"ideal 7\"\n\n# pick an arbitrary 7\na_7 = stacked_sevens[1]\nshow_image(a_7)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Goal: create a very simple baseline model using average\n# measuring distance\n\n# method 1: Mean Absolute Difference (a.k.k L1 norm) \n# dist_7_abs = (a_7 - mean7).abs().mean() \ndist_7_abs = (a_3 - mean7).abs().mean() # Note: why use a_3 - mean7 ??\n\n# method 2: Root Mean Squared Error (RRMSE)(a.k.a L2 norm)\n# dist_7_sqr = ((a_7 - mean7)**2).mean().sqrt()\ndist_7_sqr = ((a_3 - mean7)**2).mean().sqrt() # Note: why use a_3 - mean7 ??\n\n# display the distance\ndist_7_abs, dist_7_sqr","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Goal: create a very simple baseline model using average\n# measuring distance using the PyTorch loss function\nF.l1_loss(a_3.float(), mean7), F.mse_loss(a_3,mean7).sqrt()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Goal complete: create a very simple baseline model using average\n# measuring distance\n\n# The distance between our 3 and the \"ideal\" 3 < the distance to the ideal 7. \n# So our simple model will give the right prediction in this case.\n\n# now let's compare the results\nprint(f'3s: {dist_3_abs, dist_3_sqr}\\n')\nprint(f'7s {dist_7_abs, dist_7_sqr}\\n')\nprint(f'loss function: {F.l1_loss(a_3.float(), mean7), F.mse_loss(a_3,mean7).sqrt()}')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# my notes \n# Smaller values indicate closer similarity. \n# The RMSE is higher than the MAE  \n# There are possibly some pixels with relatively larger errors,...\n# pulling the RMSE up more than the MAE.\n\n### Now, is our baseline model any good? ### ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Quick lesson on numpy arrays and PyTorch tensors\n\ndata = [[1,2,3],[4,5,6]]\n\n# create a numpy array\narr = array(data)\n\n# create a PyTorch tensor\ntns = tensor(data)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Quick lesson on numpy arrays and PyTorch tensors\n\n# display the numpy array\narr","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Quick lesson on numpy arrays and PyTorch tensors\n\n# display the PyTorch tensor\ntns","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Quick lesson on numpy arrays and PyTorch tensors\n# basic operations on tensors (most are similar to numpy)\n\n# display a row\n# display index 1 (element 2) of the tensor\ntns[1]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Quick lesson on numpy arrays and PyTorch tensors\n# basic operations on tensors (most are similar to numpy)\n\n# display a column\n# show all of the first axis (index column 1)\ntns[:,1]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Quick lesson on numpy arrays and PyTorch tensors\n# basic operations on tensors (most are similar to numpy)\n\n# select part of a row or column\n# second list element [4,5,6] indexes 1 through 3 (excluding 3) so index 1 and 2\ntns[1,1:3]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Quick lesson on numpy arrays and PyTorch tensors\n# basic operations on tensors (most are similar to numpy)\n\n# perform operations on tensors\n# add one to each element of the tensor\ntns + 1","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Quick lesson on numpy arrays and PyTorch tensors\n# basic operations on tensors (most are similar to numpy)\n# check the tensor type\ntns.type()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Quick lesson on numpy arrays and PyTorch tensors\n# basic operations on tensors (most are similar to numpy)\n# automatically changes from int to float\ntns*1.5","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"### Now, is our baseline model any good? ### \n# Goal: build a metric\n# next step: check the shape of the images \n# create tensors for the 3s and 7s from the 'valid' directory\n# these tensors are used to calculate a metric measuring the quality of the model\n# the metric measures distance from an ideal image\n\nvalid_3_tens = torch.stack([tensor(Image.open(o)) for o in (path/'valid'/'3').ls()])\nvalid_3_tens = valid_3_tens.float()/255\n\nvalid_7_tens = torch.stack([tensor(Image.open(o)) for o in (path/'valid'/'7').ls()])\nvalid_7_tens = valid_7_tens.float()/255\n\n# display the number of images and the shape (remember definition of shape)\nvalid_3_tens.shape, valid_7_tens.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"### Now, is our baseline model any good? ### \n# Goal: build a metric\n# next step: write a function to decide if an aribrary number is a 3 or 7\n\n# define a function that calculates the distance between two images\n# applies the method \"elementwise\"\n# returns 1010 matrices of absolute values\n# (-1, -2) are the last two axes (horizontal and vertical dimensions) in an image\n# .mean(-1,-2) takes the mean of the last two axes  in the image\n# after taking the mean of (-1, -2) we are left with the first tensor axis\n# the first tensor axis index over all of the images\n# so we get the average intensity of all the pixels in that image\ndef mnist_distance(a,b):\n    return (a-b).abs().mean((-1,-2))\n\n# call the distance calculator function\n# result is the same value we previously calculated for the distance\nmnist_distance(a_3, mean3)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"### Now, is our baseline model any good? ### \n# Goal: build a metric\n# next step: **Apply broadcasting technque**\n\n# option 1: loop over 'valid_3_tens',the stacked ismage tensors in the validation set\n# option 2 (better): pass 'valid_3_tens' as an argument into mnist_distance function\n\n# pass the tensor for the 3s and the mean as arguments into the distance func\n# PyTorch will automatically apply the broadcasting technique\nvalid_3_dist = mnist_distance(valid_3_tens, mean3)\nvalid_3_dist, valid_3_dist.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## My Notes on Broadcasting ##\n# Shape Transformation \n# Tensor A (3x4 matrix)\nA = [\n    [1, 2, 3, 4],\n    [5, 6, 7, 8],\n    [9, 10, 11, 12]\n]\n\n# Tensor B (1D vector)\nB = [1, 2, 3, 4]\n\n# Convert B to a 2D tensor with shape (1, 4)\nB_padded = [B]  # This is now [[1, 2, 3, 4]], which is shape (1, 4)\n\n# Broadcast B to match the shape of A (3, 4)\nB_broadcast = [\n    [1, 2, 3, 4],\n    [1, 2, 3, 4],\n    [1, 2, 3, 4]\n]\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"### Now, is our baseline model any good? ### \n# Goal: build a metric\n# next step: use mnist_distance to test an arbitrary 3\n# logic: \n# if (distance between 'ideal 3' and image) < (distance to 'ideal 7') then image = 3\n\ndef is_3(x): return mnist_distance(x,mean3) < mnist_distance(x, mean7)\n\n# test an image\nis_3(a_3), is_3(a_3).float()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"### Now, is our baseline model any good? ### \n# Goal: build a metric\n# next step: use mnist_distance to test the validation set of 3s\nis_3(valid_3_tens)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"### Our baseline model is pretty good ### \n# Goal Complete: buitl a metric\n# final step: calculate the accuracy for each of the 3s and 7s\n# take the average of the function for all 3s and its inverse for all 7s\naccuracy_3s = is_3(valid_3_tens).float().mean()\naccuracy_7s = (1- is_3(valid_7_tens).float()).mean()\n\n# display the accuracy of the 3s and 7s\naccuracy_3s, accuracy_7s, (accuracy_3s+accuracy_7s)/2","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Training Process and Stochastic Gradient Descent (SGD) ##\n# Goal: build a simple loss function \n\n# build the plot_function\ndef plot_function(f, title=None, min=-2.1, max=2.1, color='b', ylim=None):\n    x = torch.linspace(min,max, 100)[:,None]\n    if ylim: plt.ylim(ylim)\n    plt.plot(x, f(x), color)\n    if title is not None: plt.title(title)\n\n# a quadratic loss function where x is a weight parameter\ndef f(x): return x**2\n\n# pass the f function as an argument into the plot_function method\n# display the loss function\nplot_function(f, title='Plot of x^2')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Training Process and Stochastic Gradient Descent (SGD) ##\n# Goal: build a simple loss function \n\n# next step: pick some random value for a parameter \n# f(-1.5) = 2.25 (x,y) = (-1.5, 2.25)\n# calculate the loss\nplot_function(f, title='Plot of x^2')\nplt.scatter(-1.5, f(-1.5), color='red')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## here is how to calculate a gradient ##\n\n# define the function\ndef f(x): return x**2\n\n# create a tensor with requires_grad=True\nx = torch.tensor(3.0, requires_grad=True)\n\n# compute the function value\ny = f(x)\n\n# compute the gradient\ny.backward()\n\n# access the gradient\ngradient = x.grad\n\nprint(f\"Gradient of f(x) = x^2 at x = {x.item()} is {gradient.item()}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Training Process and Stochastic Gradient Descent (SGD) ##\n# Goal: build a simple loss function \n\n# next step: calculate a gradient\n# pick a tensor value which we want gradients at\nxt = tensor(3.).requires_grad_()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Training Process and Stochastic Gradient Descent (SGD) ##\n# Goal: build a simple loss function \n# next step: calculate a gradient\n\n# calculate our function with the value\nyt = f(xt)\nyt","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Training Process and Stochastic Gradient Descent (SGD) ##\n# Goal: build a simple loss function \n# next step: calculate a gradient\n\n# backpropagation is the proces of calculating the derivative of each layer\nyt.backward()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Training Process and Stochastic Gradient Descent (SGD) ##\n# Goal: build a simple loss function \n\n# view the gradients\n# check the grad attribute of our tensor\nxt.grad","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Training Process and Stochastic Gradient Descent (SGD) ##\n# Goal: build a simple loss function \n\n# calculate a gradient with a vector argument\nxt = tensor([3.,4.,10.]).requires_grad_()\nxt","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Training Process and Stochastic Gradient Descent (SGD) ##\n# Goal: build a simple loss function \n\n# add sum to the function so it can take a vector\ndef f(x): return (x**2).sum()\nyt = f(xt)\nyt","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Training Process and Stochastic Gradient Descent (SGD) ##\n# Goal: build a simple loss function \n\nyt.backward()\nxt.grad","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Training Process and Stochastic Gradient Descent (SGD) ##\n# Goal: build a simple loss function \n\n## gradients only tell us the slope of our function ##\n## gradients do not tell us exactly how far to adjust the parameters ##\n# a very large slope suggests we need more adjustments\n# a very small slope may suggest that we are close to the optimal value","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Training Process and Stochastic Gradient Descent (SGD) ##\n# Goal: build a simple loss function \n# next step: stepping with a learning rate (LR)\n\n# define a function\ndef h(x): return x**2\n\n# create a tensor with requires_grad=True\n# use an initial value of 3.0\nx = torch.tensor(3.0, requires_grad=True)\n\n# define the learning rate\nlearning_rate = 0.1\n\n# perform a single gradient descent step\n# do 10 iterations \nfor i in range (10):\n    y = h(x)\n    y.backward() # compute the gradient using backpropagation\n    \n    # update x using the gradient and learning rate\n    with torch.no_grad(): # temporarily set requires_grad to False\n        x -= learning_rate * x.grad\n    \n    # zero the gradients after updating\n    x.grad.zero_()\n    \n    print(f\"Iteration {i+1}: x = {x.item()}\")\n\n# final value of x\nprint(f\"Final value x: {x.item()}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Training Process and Stochastic Gradient Descent (SGD) ##\n# Goal: Build an End-to-End SGD Example \n# model the speed of a roller coaster as it went over the top of a hump\n\n# next step: create the time tensor with values from 0 to 19 (inclusive)\n# manually measure the speed every 20 seconds\n# This tensor represents the time points at which we measure the speed of the roller coaster\ntime = torch.arange(0,20).float()\ntime","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Training Process and Stochastic Gradient Descent (SGD) ##\n# Goal: Build an End-to-End SGD Example \n# model the speed of a roller coaster as it went over the top of a hump\n\n# next step: generate the speed tensor\n# represents the speed of the roller coaster at each time point.\nspeed = torch.randn(20)*3 + 0.75*(time-9.5)**2 +1\nspeed\n\n#### explanation ###\n# torch.randn(20) * 3: \n# creates a tensor of 20 random values sampled from a standard normal distribution \n# multiplies each value by 3\n# adds a stochastic (random) component to the speed to simulate noise\n\n# 0.75 * (time - 9.5) ** 2: \n# models a quadratic function centered around time = 9.5. The ** 2 operation squares the difference between each time value and 9.5, and the 0.75 coefficient controls the curvature of the quadratic function. This simulates the expected change in speed as the roller coaster approaches the top of the hump (slowing down as it climbs and then speeding up as it descends).\n\n# + 1: \n# adds a baseline speed of 1 to all values\n# ensures that the speed remains positive and does not drop too low","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Training Process and Stochastic Gradient Descent (SGD) ##\n# Goal: Build an End-to-End SGD Example \n# model the speed of a roller coaster as it went over the top of a hump\n\n# next step: make a scatter plot of the roller coaster speed\nplt.scatter(time,speed)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Training Process and Stochastic Gradient Descent (SGD) ##\n# Goal: Build an End-to-End SGD Example \n# model the speed of a roller coaster as it went over the top of a hump\n\n# intermediate goal: find a function to fit the model\n# let's guess and start with a quadratic of the form a*(time**2) + (b*time) + c\n\n# next step: collect the parameters into one argument\n# find the best quadratic\ndef f(t, params):\n    a, b, c = params \n    return a*(t**2) + (b*t) + c","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Training Process and Stochastic Gradient Descent (SGD) ##\n# Goal: Build an End-to-End SGD Example \n# model the speed of a roller coaster as it went over the top of a hump\n\n# intermediate goal: find a function to fit the model\n\n# next step: apply SGD to minimize loss\n# for continuous data use the mean squared error \ndef mse(pred, targets): return ((preds - targets)**2).mean()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Training Process and Stochastic Gradient Descent (SGD) ##\n# Goal: Build an End-to-End SGD Example \n# model the speed of a roller coaster as it went over the top of a hump\n\n# intermediate goal: find a function to fit the model\n# apply the 7-step process\n\n# step 1: initialize the parameters\n# initialize the parameters to random values and track their gradients\n# create a 1-dimensional tensor of size 3 filled with random values\n# random values are sampled from a standard normal distribution\nparams = torch.randn(3).requires_grad_()\nparams\n\n## notes: params represent the parameters (weights) of the model that you want to optimize\n## Setting requires_grad=True computes gradients during the training process. ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Training Process and Stochastic Gradient Descent (SGD) ##\n# Goal: Build an End-to-End SGD Example \n# model the speed of a roller coaster as it went over the top of a hump\n\n# intermediate goal: find a function to fit the model\n# step 2: calculate the predictions\npreds = f(time,params)\npreds","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Training Process and Stochastic Gradient Descent (SGD) ##\n# Goal: Build an End-to-End SGD Example \n# model the speed of a roller coaster as it went over the top of a hump\n\n# intermediate goal: find a function to fit the model\n# step 2 (continued): create a function to see how close our predictions are to our targets\ndef show_preds(preds, ax=None):\n    if ax is None: \n        ax=plt.subplots()[1]\n    ax.scatter(time,speed)\n    ax.scatter(time, to_np(preds), color='red')\n    ax.set_ylim(-300,100)\nshow_preds(preds)\n\n## graphic displays negative speeds!!! So, we need to fix this","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Training Process and Stochastic Gradient Descent (SGD) ##\n# Goal: Build an End-to-End SGD Example \n# model the speed of a roller coaster as it went over the top of a hump\n\n# intermediate goal: find a function to fit the model\n# step 3: calculate the loss\nloss = mse(preds, speed)\nloss\n\n# next we need to improve the loss but first we need to know the gradients","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Training Process and Stochastic Gradient Descent (SGD) ##\n# Goal: Build an End-to-End SGD Example \n# model the speed of a roller coaster as it went over the top of a hump\n\n# intermediate goal: find a function to fit the model\n# step 4: calculate the gradients\n# calculate an approximation of how the parameters need to change\nloss.backward()\nparams.grad","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Training Process and Stochastic Gradient Descent (SGD) ##\n# Goal: Build an End-to-End SGD Example \n# model the speed of a roller coaster as it went over the top of a hump\n\n# intermediate goal: find a function to fit the model\n# step 4 (continued): choose a learning rate of 1e-5\nparams.grad * 1e-5\nparams","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Training Process and Stochastic Gradient Descent (SGD) ##\n# Goal: Build an End-to-End SGD Example \n# model the speed of a roller coaster as it went over the top of a hump\n\n# intermediate goal: find a function to fit the model\n# step 5: step the weights\n# update the parameters based on the gradients we just calculated\nlr = 1e-5\nparams.data -= lr * params.grad.data\nparams.grad = None\n\n# now see if the loos has improved\npreds = f(time, params)\nmse(preds, speed)\n\nshow_preds(preds)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Note to self: if you get the RuntimeError: \n# \"Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed).\"\n# go back to the top and rerun all the cells\n\n## Training Process and Stochastic Gradient Descent (SGD) ##\n# Goal: Build an End-to-End SGD Example \n# model the speed of a roller coaster as it went over the top of a hump\n\n# intermediate goal: find a function to fit the model\n# step 6: repeat the process\n# We need to repeat this a few times, so we'll create a function to apply one step\ndef apply_step(params, prn=True):\n    preds = f(time, params)\n    loss = mse(preds, speed)\n    loss.backward(retain_graph=True)\n    params.data -= lr * params.grad.data\n    params.grad = None\n    if prn: print(loss.item())\n    return preds\n\nfor i in range(10): apply_step(params)\n\n# def apply_step(params, prn=True):\n#     preds = f(time, params)\n#     loss = mse(preds, speed)\n#     loss.backward(retain_graph=True)  # Retain the computation graph\n#     with torch.no_grad():\n#         params.data -= lr * params.grad\n#     params.grad.zero_()\n#     if prn: print(loss.item())\n#     return preds\n\n# for i in range(10): apply_step(params)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n# Note to self: if you get the RuntimeError: \n# \"Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed).\"\n# go back to the top and rerun all the cells\n\n## Training Process and Stochastic Gradient Descent (SGD) ##\n# Goal complete: Build an End-to-End SGD Example \n# model the speed of a roller coaster as it went over the top of a hump\n\n# intermediate goal: find a function to fit the model\n# step 6 (continued): display the interations\n_,axs = plt.subplots(1,4,figsize=(12,3))\nfor ax in axs: show_preds(apply_step(params, False), ax)\nplt.tight_layout()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Training Process and Stochastic Gradient Descent (SGD) ##\n# Goal complete: Build an End-to-End SGD Example \n# Step 7: stop\n# We just decided to stop after 10 epochs arbitrarily. \n# In practice, the training and validation losses and metrics decide when to stop\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Lesson: Improving the MNIST Loss Function ##\n# Goal: update the parameters (weights + bias) using gradients to predict a 3\n\n# concatenate the images (x-variables) into a single tensor\n# change the shape of the tensor without chaning its content\ntrain_x = torch.cat([stacked_threes, stacked_sevens]).view(-1, 28*28)\ntrain_y = tensor([1]*len(threes) + [0]*len(sevens)).unsqueeze(1)\ntrain_x.shape,train_y.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Lesson: Improving the MNIST Loss Function ##\n# Goal: update the parameters (weights + bias) using gradients to predict a 3\n# a dataset in PyTorch is required to return a tuple of (x,y) when indexed. \ndset = list(zip(train_x, train_y))\nx,y = dset[0]\nx.shape,y","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Lesson: Improving the MNIST Loss Function ##\n# Goal: update the parameters (weights + bias) using gradients to predict a 3\n\nvalid_x = torch.cat([valid_3_tens, valid_7_tens]).view(-1, 28*28)\nvalid_y = tensor([1]*len(valid_3_tens) + [0]*len(valid_7_tens)).unsqueeze(1)\nvalid_dset = list(zip(valid_x,valid_y))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Lesson: Improving the MNIST Loss Function ##\n# Goal: update the parameters (weights) using gradients to predict a 3\n\n# y=w*x+b \n# The w in the equation is called the weights and the b is called the bias. \n# Together, the weights and bias make up the parameters.\n\n# create an (initially random) weight for every pixel\ndef init_params(size, std=1.0): return (torch.randn(size)*std).requires_grad_()\nweights = init_params((28*28,1))\n\n# The function weights*pixels is always equal to 0 when the pixels are equal to 0\n# So, initialize it with a randome number for more flexibility\nbias = init_params(1)\n\n# calculate a prediction for one image using y = wx + b\n(train_x[0]*weights.T).sum() + bias","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Lesson: Improving the MNIST Loss Function ##\n# Goal: update the parameters (weights) using gradients to predict a 3\n\n# matrix multiplication\n# The equation, batch@weights + bias, is one of the two fundamental equations of any neural network\ndef linear1(xb): return xb@weights + bias\npreds = linear1(train_x)\npreds","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Lesson: Improving the MNIST Loss Function ##\n# Goal: update the parameters (weights) using gradients to predict a 3\n\n# use broadcasting to check our accuracy (decide if an output represents a 3 or a 7)\n# check whether the output is greater than 0.0 to \ncorrects = (preds>0.0).float() == train_y\ncorrects","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Lesson: Improving the MNIST Loss Function ##\n# Goal: update the parameters (weights) using gradients to predict a 3\n\ncorrects.float().mean().item()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Lesson: Improving the MNIST Loss Function ##\n# Goal: update the parameters (weights) using gradients to predict a 3\n\n# make a small adjustment to the weight and observe the accuracy\n# note we have to ask PyTorch not to calculate gradients as we do this\nwith torch.no_grad(): weights[0] *=1.0001\npreds = linear1(train_x)\n((preds>0.0).float() == train_y).float().mean().item()\n\n## output \n#  small changes in the value of a weight will often not change the accuracy at all\n# ** It is not useful to use accuracy as a loss function ** #","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Lesson: Improving the MNIST Loss Function ##\n# Goal: update the parameters (weights) using gradients to predict a 3\n\n# develop a loss function \n# loss func gives us a slightly better loss when adjusting our weights result in slightly better predictions\n\n## lesson on loss ##\n# loss measures the distance between the predictions and targets\n# suppose we had three images which are 3, 7, 3 (the targets)\n# suppose the model predicts 0.9, 0.4,and 0.2 \ntrgts = tensor([1,0,1])\nprds = tensor([0.9, 0.4, 0.2])\n\n# measure how distant each prediction is from 1 if it should be 1\n# measure how distant each prediction is from 0 if it should be 0\n# take the mean of all those distances\n# note this uses the PyTorch function where()  \ndef mnist_loss(predictions, targets):\n    return torch.where(targets==1, 1-predictions, predictions).mean()\n\n# now run the loss function on the test data 3, 7, 3\n# In PyTorch, we always assume a lower value of a loss function is better. \ntorch.where(trgts==1, 1-prds, prds)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Lesson: Improving the MNIST Loss Function ##\n# Goal: update the parameters (weights) using gradients to predict a 3\n\n## lesson on loss ##\n# compute the final loss\n# we need a scalar for the final loss\n# mnist_loss takes the mean of the previous tensor\nmnist_loss(prds,trgts)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Lesson: Improving the MNIST Loss Function ##\n# Goal: update the parameters (weights) using gradients to predict a 3\n\n## lesson on loss ##\n# change our prediction for the \"false\" target from 0.2 to 0.8\n# the loss will go down\nmnist_loss(tensor([0.9,0.4,0.8]),trgts)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Lesson: Improving the MNIST Loss Function ##\n# Goal: update the parameters (weights) using gradients to predict a 3\n# The Sigmoid Function\n\n# create a sigmoid function to ensure the output is always a number between 0 and 1.","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Lesson: Improving the MNIST Loss Function ##\n# Goal: update the parameters (weights) using gradients to predict a 3","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Lesson: Improving the MNIST Loss Function ##\n# Goal: update the parameters (weights) using gradients to predict a 3","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Lesson: Improving the MNIST Loss Function ##\n# Goal: update the parameters (weights) using gradients to predict a 3","metadata":{},"execution_count":null,"outputs":[]}]}